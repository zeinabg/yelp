{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85bc97e0",
   "metadata": {},
   "source": [
    "# Unlocking Business Potential: Leveraging Bias-Aware Hierarchical Clustering for Actionable Insights from Yelp Data\n",
    "\n",
    "### Zeinab Gaeini, Debbie Hernandez, Harper Strickland\n",
    "\n",
    "## Part 3 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603e7656",
   "metadata": {},
   "source": [
    "### Using Normalized Features File, Identify Candidate Clusters for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe20ae35",
   "metadata": {},
   "source": [
    "#### This notebook has 1 output:\n",
    "1. top clusters csv file (1 row per business: cluster name, k-value, cluster number, business id)\n",
    "\n",
    "#### After running this notebook, get csv file from new local folder, rename, and move to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f657893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.ml.clustering import KMeans \n",
    "from pyspark.sql.functions import count, min, max, mean, median, std, col, lit, concat, row_number\n",
    "from pyspark.sql.types import IntegerType, FloatType, StringType, StructType, StructField\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dde67803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Suppress native-hadoop warning\n",
    "!sed -i '$a\\# Add the line for suppressing the NativeCodeLoader warning \\nlog4j.logger.org.apache.hadoop.util.NativeCodeLoader=ERROR,console' /$HADOOP_HOME/etc/hadoop/log4j.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd675a30",
   "metadata": {},
   "source": [
    "#### Terminal commands, Upload Files to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "167fcb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a directory on hdfs (repeated in case this notebook is used alone)\n",
    "! hdfs dfs -mkdir /Project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcfaa209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/Project/Restaurants_Features.csv': File exists\n",
      "Found 7 items\n",
      "-rw-r--r--   1 root supergroup   52067470 2024-06-03 02:33 /Project/CanadianPostalCodes202403.csv\n",
      "drwxr-xr-x   - root supergroup          0 2024-06-03 02:35 /Project/Restaurants_Data.csv\n",
      "drwxr-xr-x   - root supergroup          0 2024-06-03 02:35 /Project/Restaurants_Features.csv\n",
      "drwxr-xr-x   - root supergroup          0 2024-06-03 03:25 /Project/Top_Clusters.csv\n",
      "-rw-r--r--   1 root supergroup    5909393 2024-06-03 02:17 /Project/USZIPCodes202403.csv\n",
      "-rw-r--r--   1 root supergroup  118863795 2024-06-03 02:16 /Project/yelp_academic_dataset_business.json\n",
      "-rw-r--r--   1 root supergroup 5341868833 2024-06-03 02:17 /Project/yelp_academic_dataset_review.json\n"
     ]
    }
   ],
   "source": [
    "# copy csv file to hdfs: features data (normalized features of selected businesses)\n",
    "! hdfs dfs -copyFromLocal Features.csv /Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e10878a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -ls /Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee7476e",
   "metadata": {},
   "source": [
    "#### Terminal commands, Upload Files to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "663de090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the number of cores = n in `local[n]`\n",
    "conf = pyspark.SparkConf().setAll([('spark.master', 'local[4]'),\n",
    "                                   ('spark.app.name', 'Clustering')])\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3d497d",
   "metadata": {},
   "source": [
    "#### 3.1. Features CSV file to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70e5b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read features data from csv file\n",
    "path = \"hdfs:///Project/Features.csv\"\n",
    "features = spark.read.csv(path,header=True,inferSchema=True).cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de69f1d1",
   "metadata": {},
   "source": [
    "### 3.2. K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d217d9",
   "metadata": {},
   "source": [
    "#### *UPDATE HERE*\n",
    "Change list of k-values to test different range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdf577e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List K-Values for Analysis\n",
    "k_vals = [50, 100, 500, 1000, 5000, 10000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec335dbd",
   "metadata": {},
   "source": [
    "#### 3.2.1. Define Functions for K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d3748a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: dataframe with business_id and stars as 1st 2 columns,\n",
    "#    other columns are normalized features (0-1)\n",
    "def get_vectors (df):\n",
    "    vec_assembler = VectorAssembler(inputCols = df.columns[2:], outputCol='features') \n",
    "    data = vec_assembler.transform(df) \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4a3388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs: dataframe, num of clusters\n",
    "# output: dataframe columns: business_id, stars, cluster, k_value\n",
    "def get_clusters (df, num):\n",
    "    data = get_vectors(df)\n",
    "    kmeans = KMeans(featuresCol='features',k=num) \n",
    "    model = kmeans.fit(data) \n",
    "    with_labels = model.transform(data) # combine with next\n",
    "    result = with_labels.select('business_id', 'stars', 'prediction') \\\n",
    "        .withColumnRenamed('prediction', 'cluster')\n",
    "    result = result.withColumn('k_value', lit(num))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fc13fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs: cluster dataframe with 3 columns (business_id, stars, cluster)\n",
    "def cluster_stats (df):\n",
    "    result = df.groupBy('k_value', 'cluster').agg(count('business_id'), min('stars'), max('stars'),\n",
    "                                       mean('stars'), median('stars'), std('stars'))\n",
    "    result = result.orderBy(['count(business_id)'], ascending = [False])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b448f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare results for list of candidate k values\n",
    "def compare_all (df, k_list):\n",
    "    df.cache()\n",
    "    # Create schema and empty dataframe for results (1 row for each cluster)\n",
    "    columns = StructType([StructField('K_Value', # number of clusters \n",
    "                                  IntegerType(), True),\n",
    "                    StructField('Cluster', # cluster number \n",
    "                                  IntegerType(), True),\n",
    "                    StructField('Business_Count', # min businesses per cluster\n",
    "                                IntegerType(), True),\n",
    "                    StructField('Min_Stars', # min stars in cluster\n",
    "                                FloatType(), True),\n",
    "                    StructField('Max_Stars', # max stars in cluster\n",
    "                                FloatType(), True),\n",
    "                    StructField('Mean_Stars', # mean stars in cluster\n",
    "                                FloatType(), True),\n",
    "                    StructField('Median_Stars', # median stars in cluster\n",
    "                                FloatType(), True),\n",
    "                    StructField('Stars_Std', # standard deviation of stars in cluster\n",
    "                                FloatType(), True)])\n",
    " \n",
    "    final_result = spark.createDataFrame(data = [], schema = columns)\n",
    "    for i in range (len(k_list)):\n",
    "        result = cluster_stats(get_clusters(df, k_list[i]))\n",
    "        final_result = reduce(DataFrame.unionAll, [final_result,result])\n",
    "    return final_result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d3e08b",
   "metadata": {},
   "source": [
    "#### 3.2.1. Perform K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7a90279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:26:22.853 [block-manager-storage-async-thread-pool-48] ERROR org.apache.spark.storage.BlockManagerStorageEndpoint - Error in removing broadcast 28\n",
      "org.apache.spark.SparkException: Block broadcast_28 does not exist\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.blockDoesNotExistError(SparkCoreErrors.scala:318) ~[spark-core_2.12-3.5.0.jar:3.5.0]\n",
      "\tat org.apache.spark.storage.BlockInfoManager.blockInfo(BlockInfoManager.scala:269) ~[spark-core_2.12-3.5.0.jar:3.5.0]\n",
      "\tat org.apache.spark.storage.BlockInfoManager.removeBlock(BlockInfoManager.scala:544) ~[spark-core_2.12-3.5.0.jar:3.5.0]\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2095) ~[spark-core_2.12-3.5.0.jar:3.5.0]\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:2061) ~[spark-core_2.12-3.5.0.jar:3.5.0]\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3(BlockManager.scala:2033) ~[spark-core_2.12-3.5.0.jar:3.5.0]\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3$adapted(BlockManager.scala:2033) ~[spark-core_2.12-3.5.0.jar:3.5.0]\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943) ~[scala-library-2.12.18.jar:?]\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943) ~[scala-library-2.12.18.jar:?]\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431) ~[scala-library-2.12.18.jar:?]\n",
      "\tat org.apache.spark.storage.BlockManager.removeBroadcast(BlockManager.scala:2033) ~[spark-core_2.12-3.5.0.jar:3.5.0]\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$4(BlockManagerStorageEndpoint.scala:69) ~[spark-core_2.12-3.5.0.jar:3.5.0]\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23) ~[scala-library-2.12.18.jar:?]\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:101) ~[spark-core_2.12-3.5.0.jar:3.5.0]\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.18.jar:?]\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.18.jar:?]\n",
      "\tat scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.18.jar:?]\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.18.jar:?]\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.18.jar:?]\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.18.jar:?]\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) [scala-library-2.12.18.jar:?]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n",
      "\tat java.lang.Thread.run(Thread.java:829) [?:?]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# this cell takes time to run, dependent on how many k values have been chosen\n",
    "# Run K-Means for List of Ks, one row per cluster, business count, stars stats\n",
    "k_results = compare_all(features, k_vals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d1906a",
   "metadata": {},
   "source": [
    "### 3.3. Compare All Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7e3e60",
   "metadata": {},
   "source": [
    "#### 3.3.1. Define Functions to compare clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48526906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return top 10 list for clusters ordered by 'type'\n",
    "# type options: high_min, low_max, high_mean, low_mean, high_med, low_med, low_std\n",
    "def top_ten (df, type):\n",
    "    # low_std as default\n",
    "    asc = True\n",
    "    order_by = 'Stars_Std'\n",
    "    if (type=='high_min'):\n",
    "        asc = False\n",
    "        order_by = 'Min_Stars'\n",
    "    elif (type=='low_max'):\n",
    "        order_by = 'Max_Stars'\n",
    "    elif (type=='high_mean'):\n",
    "        asc = False\n",
    "        order_by = 'Mean_Stars'\n",
    "    elif (type=='low_mean'):\n",
    "        order_by = 'Mean_Stars'\n",
    "    elif (type=='high_med'):\n",
    "        asc = False\n",
    "        order_by = 'Median_Stars'\n",
    "    elif (type=='low_med'):\n",
    "        order_by = 'Median_Stars'\n",
    "    result = df.filter(col('Business_Count')>50).orderBy([order_by], ascending = [asc]).limit(10)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad9dbba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best (df):\n",
    "    # top 4 by mean stars, no duplicates\n",
    "    group_cols = ['K_Value', 'Cluster']\n",
    "    result = df.groupBy(group_cols).agg(mean(\"Mean_Stars\").alias(\"Mean_Stars\")) \\\n",
    "        .orderBy(['Mean_Stars'], ascending = [False]).limit(4)\n",
    "    w = Window().orderBy(lit('A'))\n",
    "    result = result.withColumn('Row_Number', row_number().over(w)).withColumn('Best', lit('Best'))\n",
    "    result = result.select(concat(col('Best'), lit('_'), col('Row_Number')).alias('Cluster_Name'),\n",
    "                    'K_Value', 'Cluster')\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1332b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worst (df):\n",
    "    # bottom 4 by mean stars, no duplicates\n",
    "    group_cols = ['K_Value', 'Cluster']\n",
    "    result = df.groupBy(group_cols).agg(mean(\"Mean_Stars\").alias(\"Mean_Stars\")) \\\n",
    "        .orderBy(['Mean_Stars'], ascending = [True]).limit(4)\n",
    "    w = Window().orderBy(lit('A'))\n",
    "    result = result.withColumn('Row_Number', row_number().over(w)).withColumn('Worst', lit('Worst'))\n",
    "    result = result.select(concat(col('Worst'), lit('_'), col('Row_Number')).alias('Cluster_Name'),\n",
    "                    'K_Value', 'Cluster')\n",
    "    return result\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bd3c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare results for list of candidate k values\n",
    "def compare_k (df):\n",
    "    df.cache()\n",
    "    # Create schema and empty dataframe for results (1 row for each cluster)\n",
    "    columns = StructType([StructField('K_Value', # number of clusters \n",
    "                                  IntegerType(), True),\n",
    "                    StructField('Cluster', # cluster number \n",
    "                                  IntegerType(), True),\n",
    "                    StructField('Mean_Stars', # mean stars in cluster\n",
    "                                FloatType(), True)])\n",
    "    # DF of all top 10 lists\n",
    "    allDF = spark.createDataFrame(data = [], schema = columns)\n",
    "\n",
    "    types = ['high_min', 'low_max', 'high_mean', 'low_mean', 'high_med', 'low_med', 'low_std']\n",
    "    type_labels = ['Highest Minimum Stars', 'Lowest Maximum Stars',\n",
    "               'Highest Mean Stars', 'Lowest Mean Stars',\n",
    "               'Highest Median Stars', 'Lowest Median Stars',\n",
    "               'Lowest Standard Deviation of Stars']\n",
    "    for i in range (len(types)):\n",
    "        print(type_labels[i])\n",
    "        result = top_ten(k_results, types[i])\n",
    "        result.show()\n",
    "        allDF = reduce(DataFrame.unionAll, [allDF,result.select('K_Value', 'Cluster', 'Mean_Stars')])\n",
    "\n",
    "    # get best and worst of allDF, combine into final_result\n",
    "    final_result = get_best(allDF).union(get_worst(allDF))\n",
    "    return final_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b37f47f",
   "metadata": {},
   "source": [
    "#### 3.3.2. Perform comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a714c7",
   "metadata": {},
   "source": [
    "Print Top 10 Lists and save Top 'Best' and 'Worst' Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56206715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Minimum Stars\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------+---------+---------+------------------+------------+-------------------+\n",
      "|K_Value|Cluster|Business_Count|Min_Stars|Max_Stars|        Mean_Stars|Median_Stars|          Stars_Std|\n",
      "+-------+-------+--------------+---------+---------+------------------+------------+-------------------+\n",
      "|    500|    439|            60|      3.5|      4.5| 4.166666666666667|         4.0|0.30065841120113157|\n",
      "|    500|    132|            57|      3.5|      5.0| 4.166666666666667|         4.0|0.35773760003135024|\n",
      "|    500|     43|            55|      3.5|      5.0| 4.181818181818182|         4.0| 0.3646870343138524|\n",
      "|    100|     59|           160|      3.0|      4.5|             3.925|         4.0|  0.438694461087303|\n",
      "|    500|    107|            59|      3.0|      5.0|3.9915254237288136|         4.0|0.40991510289798927|\n",
      "|   1000|    119|            73|      3.0|      4.5|3.8698630136986303|         4.0|  0.353822354793903|\n",
      "|    500|    265|            68|      3.0|      5.0|               4.0|         4.0| 0.4652073843435434|\n",
      "|   1000|    271|            58|      3.0|      5.0| 4.137931034482759|         4.0| 0.5112171874107659|\n",
      "|    500|     55|            82|      3.0|      5.0| 4.060975609756097|         4.0|0.44714625926596924|\n",
      "|   1000|    655|            58|      3.0|      4.5|3.5344827586206895|         3.5|0.45751108667917745|\n",
      "+-------+-------+--------------+---------+---------+------------------+------------+-------------------+\n",
      "\n",
      "Lowest Maximum Stars\n",
      "+-------+-------+--------------+---------+---------+------------------+------------+-------------------+\n",
      "|K_Value|Cluster|Business_Count|Min_Stars|Max_Stars|        Mean_Stars|Median_Stars|          Stars_Std|\n",
      "+-------+-------+--------------+---------+---------+------------------+------------+-------------------+\n",
      "|    500|    457|            83|      1.0|      3.5|2.1626506024096384|         2.0| 0.6054046909689366|\n",
      "|   5000|   2188|            51|      1.0|      3.5|               2.0|         2.0| 0.6633249580710799|\n",
      "|   1000|    558|            57|      1.0|      3.5|1.8771929824561404|         2.0| 0.5284808226888414|\n",
      "|    500|    392|            73|      1.0|      3.5|2.0547945205479454|         2.0|0.49694653632916946|\n",
      "|   1000|    125|            53|      1.0|      3.5| 1.990566037735849|         2.0| 0.6162117989231263|\n",
      "|    500|    177|            65|      1.0|      3.5|2.1076923076923078|         2.0| 0.5624465786598347|\n",
      "|   1000|    228|            53|      1.0|      3.5| 2.009433962264151|         2.0| 0.5674721785601191|\n",
      "|   1000|    567|            53|      1.0|      3.5|2.1226415094339623|         2.0| 0.5875770029516793|\n",
      "|  10000|   1099|            55|      1.0|      4.0| 1.981818181818182|         2.0| 0.6801663492310264|\n",
      "|   5000|    241|            73|      1.5|      4.0|2.4863013698630136|         2.5| 0.7816142605909953|\n",
      "+-------+-------+--------------+---------+---------+------------------+------------+-------------------+\n",
      "\n",
      "Highest Mean Stars\n",
      "+-------+-------+--------------+---------+---------+-----------------+------------+------------------+\n",
      "|K_Value|Cluster|Business_Count|Min_Stars|Max_Stars|       Mean_Stars|Median_Stars|         Stars_Std|\n",
      "+-------+-------+--------------+---------+---------+-----------------+------------+------------------+\n",
      "|   1000|     80|            51|      3.0|      5.0|4.362745098039215|         4.5|0.5298908507659764|\n",
      "|    500|    317|            62|      3.0|      5.0| 4.32258064516129|         4.5|0.5588100222175014|\n",
      "|   1000|    129|            51|      2.0|      5.0|4.313725490196078|         4.5|0.7208382919471293|\n",
      "|    500|    452|           110|      2.5|      5.0|4.295454545454546|         4.5|0.5877005983728221|\n",
      "|    100|     29|           171|      2.5|      5.0|4.271929824561403|         4.5|0.5455987706876285|\n",
      "|    500|    140|           128|      2.5|      5.0|       4.26953125|         4.5|0.5394016214769551|\n",
      "|    100|      9|           267|      1.5|      5.0|4.262172284644195|         4.5|0.5590941252446733|\n",
      "|   1000|    267|            66|      2.5|      5.0|4.234848484848484|         4.5|0.6090326312446207|\n",
      "|    100|     68|           218|      2.5|      5.0| 4.23394495412844|         4.5|0.5454833705577712|\n",
      "|     50|     29|           507|      1.5|      5.0|4.216962524654832|         4.5|0.6137974236252132|\n",
      "+-------+-------+--------------+---------+---------+-----------------+------------+------------------+\n",
      "\n",
      "Lowest Mean Stars\n",
      "+-------+-------+--------------+---------+---------+------------------+------------+------------------+\n",
      "|K_Value|Cluster|Business_Count|Min_Stars|Max_Stars|        Mean_Stars|Median_Stars|         Stars_Std|\n",
      "+-------+-------+--------------+---------+---------+------------------+------------+------------------+\n",
      "|   1000|    558|            57|      1.0|      3.5|1.8771929824561404|         2.0|0.5284808226888414|\n",
      "|  10000|   1099|            55|      1.0|      4.0| 1.981818181818182|         2.0|0.6801663492310264|\n",
      "|   1000|    125|            53|      1.0|      3.5| 1.990566037735849|         2.0|0.6162117989231263|\n",
      "|  10000|    499|            51|      1.0|      4.0|               2.0|         2.0|  0.58309518948453|\n",
      "|   5000|   2188|            51|      1.0|      3.5|               2.0|         2.0|0.6633249580710799|\n",
      "|   1000|    228|            53|      1.0|      3.5| 2.009433962264151|         2.0|0.5674721785601191|\n",
      "|   5000|   2168|            72|      1.0|      4.5| 2.013888888888889|         2.0|0.6498765983740614|\n",
      "|   1000|    144|           184|      1.0|      4.5|2.0244565217391304|         2.0|0.6429098197653205|\n",
      "|   5000|     33|            55|      1.0|      4.0| 2.036363636363636|         2.0|0.6372288490492297|\n",
      "|  10000|    221|            59|      1.0|      4.5|2.0508474576271185|         2.0|0.6739929605065548|\n",
      "+-------+-------+--------------+---------+---------+------------------+------------+------------------+\n",
      "\n",
      "Highest Median Stars\n",
      "+-------+-------+--------------+---------+---------+------------------+------------+------------------+\n",
      "|K_Value|Cluster|Business_Count|Min_Stars|Max_Stars|        Mean_Stars|Median_Stars|         Stars_Std|\n",
      "+-------+-------+--------------+---------+---------+------------------+------------+------------------+\n",
      "|    500|      6|           153|      2.0|      5.0|  4.11437908496732|         4.5|0.6822387971977179|\n",
      "|    100|      9|           267|      1.5|      5.0| 4.262172284644195|         4.5|0.5590941252446733|\n",
      "|   1000|    244|            83|      1.5|      5.0|  4.13855421686747|         4.5|0.7296560040319038|\n",
      "|    100|     47|           228|      1.5|      5.0|4.1118421052631575|         4.5|0.7432457284403912|\n",
      "|     50|     29|           507|      1.5|      5.0| 4.216962524654832|         4.5|0.6137974236252132|\n",
      "|    100|     68|           218|      2.5|      5.0|  4.23394495412844|         4.5|0.5454833705577712|\n",
      "|   1000|    242|            76|      2.0|      5.0| 4.157894736842105|         4.5|0.6890114963520297|\n",
      "|    100|     29|           171|      2.5|      5.0| 4.271929824561403|         4.5|0.5455987706876285|\n",
      "|    500|    452|           110|      2.5|      5.0| 4.295454545454546|         4.5|0.5877005983728221|\n",
      "|   1000|    267|            66|      2.5|      5.0| 4.234848484848484|         4.5|0.6090326312446207|\n",
      "+-------+-------+--------------+---------+---------+------------------+------------+------------------+\n",
      "\n",
      "Lowest Median Stars\n",
      "+-------+-------+--------------+---------+---------+------------------+------------+------------------+\n",
      "|K_Value|Cluster|Business_Count|Min_Stars|Max_Stars|        Mean_Stars|Median_Stars|         Stars_Std|\n",
      "+-------+-------+--------------+---------+---------+------------------+------------+------------------+\n",
      "|   1000|     79|           130|      1.0|      4.5| 2.146153846153846|         2.0|0.6332093529592049|\n",
      "|   5000|    224|           108|      1.0|      4.5|2.0972222222222223|         2.0|0.6713716373137967|\n",
      "|  10000|    743|            94|      1.0|      4.5| 2.154255319148936|         2.0|0.6761155486944935|\n",
      "|   5000|   2168|            72|      1.0|      4.5| 2.013888888888889|         2.0|0.6498765983740614|\n",
      "|    500|    206|           248|      1.0|      4.5|2.1129032258064515|         2.0|0.6638091211344512|\n",
      "|   5000|    782|            67|      1.0|      4.0|2.2388059701492535|         2.0|0.6705843728986968|\n",
      "|  10000|   1055|            62|      1.0|      4.0|2.2580645161290325|         2.0|0.6879640638481659|\n",
      "|   5000|     33|            55|      1.0|      4.0| 2.036363636363636|         2.0|0.6372288490492297|\n",
      "|    100|      1|           665|      1.0|      4.5|2.1406015037593984|         2.0|0.6773039210751397|\n",
      "|   5000|   2188|            51|      1.0|      3.5|               2.0|         2.0|0.6633249580710799|\n",
      "+-------+-------+--------------+---------+---------+------------------+------------+------------------+\n",
      "\n",
      "Lowest Standard Deviation of Stars\n",
      "+-------+-------+--------------+---------+---------+------------------+------------+-------------------+\n",
      "|K_Value|Cluster|Business_Count|Min_Stars|Max_Stars|        Mean_Stars|Median_Stars|          Stars_Std|\n",
      "+-------+-------+--------------+---------+---------+------------------+------------+-------------------+\n",
      "|    500|    439|            60|      3.5|      4.5| 4.166666666666667|         4.0|0.30065841120113157|\n",
      "|   1000|    119|            73|      3.0|      4.5|3.8698630136986303|         4.0|  0.353822354793903|\n",
      "|    500|    132|            57|      3.5|      5.0| 4.166666666666667|         4.0|0.35773760003135024|\n",
      "|    500|    416|            61|      3.0|      5.0| 4.163934426229508|         4.0| 0.3619543188881003|\n",
      "|    500|    398|            52|      3.0|      4.5| 3.923076923076923|         4.0| 0.3622453856945781|\n",
      "|    500|     43|            55|      3.5|      5.0| 4.181818181818182|         4.0| 0.3646870343138524|\n",
      "|    500|     41|            53|      3.0|      4.5|3.9150943396226414|         4.0|0.37641845904989546|\n",
      "|    500|    124|            60|      3.0|      5.0| 4.141666666666667|         4.0| 0.3805845934458745|\n",
      "|    500|     70|            89|      2.5|      4.5| 3.853932584269663|         4.0|0.38584505244253436|\n",
      "|    500|    112|            55|      3.0|      4.5| 4.027272727272727|         4.0|  0.389897681475797|\n",
      "+-------+-------+--------------+---------+---------+------------------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_clusters = compare_k(k_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eb7e794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 446:========================================>               (8 + 3) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------+\n",
      "|Cluster_Name|K_Value|Cluster|\n",
      "+------------+-------+-------+\n",
      "|      Best_1|   1000|     80|\n",
      "|      Best_2|    500|    317|\n",
      "|      Best_3|   1000|    129|\n",
      "|      Best_4|    500|    452|\n",
      "|     Worst_1|   1000|    558|\n",
      "|     Worst_2|  10000|   1099|\n",
      "|     Worst_3|   1000|    125|\n",
      "|     Worst_4|   5000|   2188|\n",
      "+------------+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_clusters.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb5d8cf",
   "metadata": {},
   "source": [
    "### 3.4. Save 'Best' and 'Worst' with Businesses for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01993323",
   "metadata": {},
   "source": [
    "#### 3.4.1. Define Functions to compile dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6af2e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get business_id list for a specific cluster\n",
    "def get_id_list (k, cluster):\n",
    "    bus_list = get_clusters(features, k).filter(col('cluster')==cluster).select('k_value', 'cluster', 'business_id')\n",
    "    return bus_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79187cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_id_list (k_cl_df):\n",
    "    # create list of tuples (name, k, c)\n",
    "    name = [str(row['Cluster_Name']) for row in k_cl_df.collect()]\n",
    "    k = [int(row['K_Value']) for row in k_cl_df.collect()]\n",
    "    c = [int(row['Cluster']) for row in k_cl_df.collect()]\n",
    "    list_ = [name, k, c]\n",
    "    k_cl_list = [list(tup) for tup in zip(*list_)]\n",
    "    \n",
    "    # Create schema and empty dataframe for results (1 row for each cluster)\n",
    "    columns = StructType([StructField('Cluster_Name', # number of clusters \n",
    "                                StringType(), True),\n",
    "                    StructField('K_Value', # number of clusters \n",
    "                                IntegerType(), True),\n",
    "                    StructField('Cluster', # cluster number \n",
    "                                IntegerType(), True),\n",
    "                    StructField('Business_ID', # min businesses per cluster\n",
    "                                StringType(), True)])\n",
    " \n",
    "    final_result = spark.createDataFrame(data = [], schema = columns)\n",
    "    for i in range (len(k_cl_list)):\n",
    "        result = get_id_list(k_cl_list[i][1], k_cl_list[i][2])\n",
    "        result = result.select(lit(k_cl_list[i][0]).alias('Cluster_Name'),\n",
    "                    'k_value', 'cluster', 'business_id')\n",
    "        final_result = reduce(DataFrame.unionAll, [final_result,result])\n",
    "    return final_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db102e1d",
   "metadata": {},
   "source": [
    "#### 3.4.2. Compile Top Clusters Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec7e84c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# this step takes the most time (looking up all businesses in each of the top clusters)\n",
    "top_ids = combine_id_list(top_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ae63005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name to save in HDFS\n",
    "save_path = 'hdfs:///Project/Top_Clusters.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3730e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_ids.coalesce(1).write.csv(save_path, mode='overwrite', header='true')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99c3294",
   "metadata": {},
   "source": [
    "#### End Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ab274b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End Spark Session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3b41ce",
   "metadata": {},
   "source": [
    "Copy csv file from HDFS to local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceabc4da",
   "metadata": {},
   "source": [
    "#### *UPDATE HERE:*\n",
    "\n",
    "##### (filename in Project folder should match name of .csv folders where file was saved in HDFS at top of notebook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebca654c",
   "metadata": {
    "id": "9i_FdeZVInfz"
   },
   "outputs": [],
   "source": [
    "# UPDATE with folder name in HDFS (see output above)\n",
    "! hdfs dfs -copyToLocal /Project/Top_Clusters.csv Clusters_CSV_Folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce8cc22",
   "metadata": {},
   "source": [
    "##### after notebook runs, find the new file named “part-00000-…” in folder named “Clusters_CSV_Folder” Rename the “part-…” file as “Top_Clusters.csv” and move it into local folder.\n",
    "\n",
    "##### Proceed to next notebook (4-Analysis.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e83b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
